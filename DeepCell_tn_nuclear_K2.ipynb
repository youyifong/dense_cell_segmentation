{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "330e05e3",
   "metadata": {},
   "source": [
    "**Further train DeepCell Models with K's Data and Make Predictions**<br>\n",
    "The model has three heads: inner distance, outer distance, and fgbg. It works on tensorflow 2.7.1.<br>\n",
    "We first train a model with nucleus data from the Tissuenet V1.0 dataset and save the model. <br>\n",
    "We then further train the model via deepcell.training.train_model_sample, which allows arbitrary size images and uses window_size to control patch size.\n",
    "There are a total of 12,574 training instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a72e9c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_model_path = 'tn1.0_nuclear_20221102.h5' # the model trained with nucleus data from the Tissuenet V1.0 dataset\n",
    "model_path = 'tn1.0_nuclear_K.h5' # the model trained with nucleus data from the Tissuenet V1.0 dataset\n",
    "epochs=60 # 7 sec per epoch\n",
    "label=\"nuclear_K\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90f2183c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1\n",
      "0.11.1\n"
     ]
    }
   ],
   "source": [
    "import syotil\n",
    "\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from timeit import default_timer\n",
    "import os\n",
    "\n",
    "import deepcell\n",
    "from deepcell import image_generators\n",
    "from deepcell.utils.train_utils import rate_scheduler, get_callbacks, count_gpus\n",
    "from deepcell_toolbox.utils import resize, tile_image, untile_image\n",
    "from deepcell_toolbox.deep_watershed import deep_watershed\n",
    "from deepcell.losses import weighted_categorical_crossentropy\n",
    "from deepcell.model_zoo.panopticnet import PanopticNet\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.losses import MSE\n",
    "\n",
    "print(tf.__version__)\n",
    "print(deepcell.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "938b06af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "['images/training/M872956_JML_Position9_CD3_train_img.png', 'images/training/M872956_JML_Position8_CD3_train_img.png', 'images/training/M926910_CFL_Position7_CD3_train_img.png', 'images/training/M926910_CFL_Position13_CD3_train_img.png', 'images/training/M872956_JML_Position8_CD8_train_img.png', 'images/training/M872956_JML_Position10_CD3_train_img.png', 'images/training/M872956_JML_Position8_CD4_train_img.png']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "INPUT_PATH=\"images/training/\"\n",
    "FILENAMES = glob.glob(INPUT_PATH+\"*_img.png\")\n",
    "print(len(FILENAMES))\n",
    "print(FILENAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bac06f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 18:38:17.808095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13832 MB memory:  -> device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 7.0\n",
      "2022-11-02 18:38:17.809568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 14208 MB memory:  -> device: 1, name: Tesla V100-PCIE-16GB, pci bus id: 0000:06:00.0, compute capability: 7.0\n",
      "2022-11-02 18:38:17.810771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 14208 MB memory:  -> device: 2, name: Tesla V100-PCIE-16GB, pci bus id: 0000:08:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 1040, 1159, 1)\n",
      "(7, 1040, 1159, 1)\n"
     ]
    }
   ],
   "source": [
    "imgs = [io.imread(CURR_IM_NAME)[:,:,0] for CURR_IM_NAME in FILENAMES]\n",
    "X_train = tf.stack(imgs)\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "print(X_train.shape)\n",
    "\n",
    "masks = [io.imread(CURR_IM_NAME.replace(\"img\",\"masks\")) for CURR_IM_NAME in FILENAMES]\n",
    "y_train = tf.stack(masks)\n",
    "y_train = np.expand_dims(y_train, axis=-1)\n",
    "print(y_train.shape)\n",
    "\n",
    "# np.savez(\"K_training_data\", X=X_train, y=y_train) # objects to save need to be key value pairs\n",
    "# train_dict, test_dict = get_data(\"K_training_data.npz\", test_size=test_size, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a53a38a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 1783)\n",
      "(7, 1599, 1783, 1)\n"
     ]
    }
   ],
   "source": [
    "# resize image\n",
    "image_mpp=1\n",
    "shape = X_train.shape\n",
    "scale_factor = image_mpp / 0.65\n",
    "new_shape = (int(shape[1] * scale_factor),\n",
    "             int(shape[2] * scale_factor))\n",
    "\n",
    "X_train = resize(X_train, new_shape, data_format='channels_last')\n",
    "y_train = resize(y_train, new_shape, data_format='channels_last')\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ef13c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images/training/M872956_JML_Position9_CD3_train_img.png',\n",
       " 'images/training/M872956_JML_Position8_CD3_train_img.png',\n",
       " 'images/training/M926910_CFL_Position7_CD3_train_img.png',\n",
       " 'images/training/M926910_CFL_Position13_CD3_train_img.png',\n",
       " 'images/training/M872956_JML_Position8_CD8_train_img.png',\n",
       " 'images/training/M872956_JML_Position10_CD3_train_img.png',\n",
       " 'images/training/M872956_JML_Position8_CD4_train_img.png']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILENAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "248e9b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31175/3883742908.py:4: UserWarning: images/training/M872956_JML_Position9_CD3_train_resize_img.png is a low contrast image\n",
      "  io.imsave(FILENAMES[i].replace(\"_img\",\"_resize_img\"), X_train[i,:,:,0])\n",
      "Lossy conversion from int32 to uint8. Range [0, 254]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "/tmp/ipykernel_31175/3883742908.py:4: UserWarning: images/training/M872956_JML_Position8_CD3_train_resize_img.png is a low contrast image\n",
      "  io.imsave(FILENAMES[i].replace(\"_img\",\"_resize_img\"), X_train[i,:,:,0])\n",
      "Lossy conversion from int32 to uint8. Range [0, 254]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "/tmp/ipykernel_31175/3883742908.py:4: UserWarning: images/training/M926910_CFL_Position7_CD3_train_resize_img.png is a low contrast image\n",
      "  io.imsave(FILENAMES[i].replace(\"_img\",\"_resize_img\"), X_train[i,:,:,0])\n",
      "Lossy conversion from int32 to uint8. Range [0, 254]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "/tmp/ipykernel_31175/3883742908.py:4: UserWarning: images/training/M926910_CFL_Position13_CD3_train_resize_img.png is a low contrast image\n",
      "  io.imsave(FILENAMES[i].replace(\"_img\",\"_resize_img\"), X_train[i,:,:,0])\n",
      "Lossy conversion from int32 to uint8. Range [0, 254]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "/tmp/ipykernel_31175/3883742908.py:4: UserWarning: images/training/M872956_JML_Position8_CD8_train_resize_img.png is a low contrast image\n",
      "  io.imsave(FILENAMES[i].replace(\"_img\",\"_resize_img\"), X_train[i,:,:,0])\n",
      "Lossy conversion from int32 to uint8. Range [0, 254]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "/tmp/ipykernel_31175/3883742908.py:4: UserWarning: images/training/M872956_JML_Position10_CD3_train_resize_img.png is a low contrast image\n",
      "  io.imsave(FILENAMES[i].replace(\"_img\",\"_resize_img\"), X_train[i,:,:,0])\n",
      "Lossy conversion from int32 to uint8. Range [0, 250]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "/tmp/ipykernel_31175/3883742908.py:4: UserWarning: images/training/M872956_JML_Position8_CD4_train_resize_img.png is a low contrast image\n",
      "  io.imsave(FILENAMES[i].replace(\"_img\",\"_resize_img\"), X_train[i,:,:,0])\n",
      "Lossy conversion from int32 to uint8. Range [0, 254]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "# write resized images and masks. saved to images/training\n",
    "import cv2\n",
    "for i in range(7):\n",
    "    io.imsave(FILENAMES[i].replace(\"_img\",\"_resize_img\"), X_train[i,:,:,0])    \n",
    "    io.imsave(FILENAMES[i].replace(\"_img\",\"_resize_masks\"), y_train[i,:,:,0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fd0c48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 1599, 1783, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2d16287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-02 18:59:08.777994: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    }
   ],
   "source": [
    "prediction_model = PanopticNet(\n",
    "    backbone='resnet50',\n",
    "    norm_method='whole_image',\n",
    "    num_semantic_classes=[1, 1], # inner distance, outer distance\n",
    "    input_shape= (512,512,1)\n",
    ")\n",
    "\n",
    "prediction_model.load_weights(starting_model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e3388b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 1599, 1783, 1)\n",
      "(20, 512, 512, 1)\n",
      "{'batches': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'x_starts': [0, 0, 0, 0, 0, 384, 384, 384, 384, 384, 768, 768, 768, 768, 768, 1152, 1152, 1152, 1152, 1152], 'x_ends': [512, 512, 512, 512, 512, 896, 896, 896, 896, 896, 1280, 1280, 1280, 1280, 1280, 1664, 1664, 1664, 1664, 1664], 'y_starts': [0, 384, 768, 1152, 1536, 0, 384, 768, 1152, 1536, 0, 384, 768, 1152, 1536, 0, 384, 768, 1152, 1536], 'y_ends': [512, 896, 1280, 1664, 2048, 512, 896, 1280, 1664, 2048, 512, 896, 1280, 1664, 2048, 512, 896, 1280, 1664, 2048], 'overlaps_x': [(0, 128), (0, 128), (0, 128), (0, 128), (0, 128), (128, 128), (128, 128), (128, 128), (128, 128), (128, 128), (128, 128), (128, 128), (128, 128), (128, 128), (128, 128), (128, 0), (128, 0), (128, 0), (128, 0), (128, 0)], 'overlaps_y': [(0, 128), (128, 128), (128, 128), (128, 128), (128, 0), (0, 128), (128, 128), (128, 128), (128, 128), (128, 0), (0, 128), (128, 128), (128, 128), (128, 128), (128, 0), (0, 128), (128, 128), (128, 128), (128, 128), (128, 0)], 'stride_x': 384, 'stride_y': 384, 'tile_size_x': 512, 'tile_size_y': 512, 'stride_ratio': 0.75, 'image_shape': (1, 1664, 2048, 1), 'dtype': dtype('int32'), 'pad_x': (33, 32), 'pad_y': (133, 132)}\n"
     ]
    }
   ],
   "source": [
    "# get tile info \n",
    "from deepcell.applications import NuclearSegmentation\n",
    "app1 = NuclearSegmentation(prediction_model)\n",
    "\n",
    "y, tile_info = app1._tile_input(np.expand_dims(X_train[0,...],0))\n",
    "print(X_train.shape)\n",
    "print(y.shape)\n",
    "print(tile_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73d0c5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12574\n"
     ]
    }
   ],
   "source": [
    "seed=0\n",
    "min_objects = 2\n",
    "\n",
    "transforms = ['inner-distance', 'outer-distance', 'fgbg']\n",
    "transforms_kwargs = {'outer-distance': {'erosion_width': 0}}\n",
    "\n",
    "# use augmentation for training but not validation\n",
    "# datagen = image_generators.CroppingDataGenerator(\n",
    "#     crop_size=(512, 512),\n",
    "datagen = image_generators.SemanticDataGenerator(\n",
    "    rotation_range=180,\n",
    "    fill_mode='reflect',\n",
    "    zoom_range=(0.75, 1.25),\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "\n",
    "datagen_val = image_generators.CroppingDataGenerator()\n",
    "\n",
    "batch_size = 4 # 8 causes memory outage\n",
    "\n",
    "train_data = datagen.flow(\n",
    "    {'X': X_train, 'y': y_train},\n",
    "    seed=seed,\n",
    "    transforms=transforms,\n",
    "    transforms_kwargs=transforms_kwargs,\n",
    "    min_objects=min_objects,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "val_data = None\n",
    "# datagen_val.flow(\n",
    "#     {'X': X_val, 'y': y_val},\n",
    "#     seed=seed,\n",
    "#     transforms=transforms,\n",
    "#     transforms_kwargs=transforms_kwargs,\n",
    "#     min_objects=min_objects,\n",
    "#     batch_size=batch_size)\n",
    "\n",
    "# get number of training and validation instances\n",
    "\n",
    "cnts_train = [np.max(y_train[i,...]) for i in range(y_train.shape[0])]\n",
    "print(np.sum(cnts_train)) # total number of training instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7034669",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31175/708727814.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tmp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/app/software/DeepCell/0.11.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/deepcell/image_generators/semantic.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# The transformation of images is not under thread lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;31m# so it can be done in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batches_of_transformed_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/software/DeepCell/0.11.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/deepcell/image_generators/semantic.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;31m# _transform_labels expects batch dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0my_semantic_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;31m# initialize batch_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/software/DeepCell/0.11.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/deepcell/image_generators/semantic.py\u001b[0m in \u001b[0;36m_transform_labels\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mtransform_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 y_transform = _transform_masks(y_current, transform,\n\u001b[0m\u001b[1;32m    160\u001b[0m                                                \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                                                **transform_kwargs)\n",
      "\u001b[0;32m/app/software/DeepCell/0.11.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/deepcell/image_generators/__init__.py\u001b[0m in \u001b[0;36m_transform_masks\u001b[0;34m(y, transform, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0my_transform\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_distance_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdistance_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0my_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/app/software/DeepCell/0.11.1-foss-2021b-CUDA-11.4.1/lib/python3.9/site-packages/deepcell/utils/transform_utils.py\u001b[0m in \u001b[0;36mouter_distance_transform_2d\u001b[0;34m(mask, bins, erosion_width, normalize)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mlabel_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mregionprops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mlabeled_distance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_matrix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0mnormalized_distance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabeled_distance\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_distance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mdistance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel_matrix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalized_distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_tmp, y_tmp=train_data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b79383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=5\n",
    "plt.subplot(1, 2, 1) # row 1, col 2 index 1\n",
    "io.imshow(X_train[i,:,:,0])\n",
    "plt.subplot(1, 2, 2) # row 1, col 2 index 1\n",
    "# tmp = syotil.masks_to_outlines(y_train[i,:,:,0]); io.imshow(tmp)\n",
    "io.imshow(y_train[i,:,:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f242c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape\n",
    "print(1783/512)\n",
    "1599/512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9591736f",
   "metadata": {},
   "source": [
    "**The two cells below define and train the model.** They can be skipped if a trained model will be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebc07f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_classes = [1, 1, 2] # inner distance, outer distance, fgbg\n",
    "\n",
    "model = PanopticNet(\n",
    "    backbone='resnet50',\n",
    "    input_shape=(512,512,1),\n",
    "    norm_method='whole_image',\n",
    "    num_semantic_classes=semantic_classes)\n",
    "\n",
    "lr = 1e-4\n",
    "optimizer = Adam(lr=lr, clipnorm=0.001)\n",
    "lr_sched = rate_scheduler(lr=lr, decay=0.99)\n",
    "\n",
    "# Create a dictionary of losses for each semantic head\n",
    "\n",
    "def semantic_loss(n_classes):\n",
    "    def _semantic_loss(y_pred, y_true):\n",
    "        if n_classes > 1:\n",
    "            return 0.01 * weighted_categorical_crossentropy(\n",
    "                y_pred, y_true, n_classes=n_classes)\n",
    "        return MSE(y_pred, y_true)\n",
    "    return _semantic_loss\n",
    "\n",
    "loss = {}\n",
    "\n",
    "# Give losses for all of the semantic heads\n",
    "for layer in model.layers:\n",
    "    if layer.name.startswith('semantic_'):\n",
    "        n_classes = layer.output_shape[-1]\n",
    "        loss[layer.name] = semantic_loss(n_classes)\n",
    "        \n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "model.load_weights(starting_model_path, by_name=True)\n",
    "\n",
    "[(layer.name, layer.output_shape) for layer in filter(lambda x: x.name.startswith('semantic_'), model.layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fab4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "print('Training on', count_gpus(), 'GPUs.')\n",
    "\n",
    "train_callbacks = get_callbacks(\n",
    "    model_path,\n",
    "    lr_sched=lr_sched,\n",
    "#     monitor=\"val_loss\",\n",
    "    monitor='loss', # training loss\n",
    "    verbose=1)          \n",
    "            \n",
    "loss_history = model.fit(\n",
    "    train_data,\n",
    "    steps_per_epoch=train_data.y.shape[0] // batch_size,\n",
    "    epochs=2, \n",
    "#     validation_data=val_data,\n",
    "#     validation_steps=val_data.y.shape[0] // batch_size,\n",
    "    callbacks=train_callbacks)\n",
    "\n",
    "model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f3f44b",
   "metadata": {},
   "source": [
    "<B>Make predictions on Nuclear test dataset.</B> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf36df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model = PanopticNet(\n",
    "    backbone='resnet50',\n",
    "    norm_method='whole_image',\n",
    "    num_semantic_classes=[1, 1], # inner distance, outer distance\n",
    "    input_shape= X_train.shape[1:]\n",
    ")\n",
    "\n",
    "prediction_model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42277d43",
   "metadata": {},
   "source": [
    "**Make prediction on K's data.**<br>\n",
    "Using NuclearSegmentation allows setting image_mpp, which has a substantial influence on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a766dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcell.applications import NuclearSegmentation\n",
    "app = NuclearSegmentation(prediction_model)\n",
    "[(layer.name, layer.output_shape) for layer in filter(lambda x: x.name.startswith('semantic_'), app.model.layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20301e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "INPUT_PATH=\"images/test/\"\n",
    "FILENAMES = [f for f in os.listdir(\"images/training/testimages\")]\n",
    "print(FILENAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31fa740",
   "metadata": {},
   "outputs": [],
   "source": [
    "APs={}\n",
    "for CURR_IM_NAME in FILENAMES:\n",
    "    im0 = io.imread(os.path.join(INPUT_PATH, CURR_IM_NAME))\n",
    "    mask_true=io.imread(os.path.join(INPUT_PATH, CURR_IM_NAME.replace(\"img\",\"masks\")))\n",
    "\n",
    "    x = np.expand_dims(im0, axis=-1)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    y, tile_info = app._tile_input(x)\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(tile_info)\n",
    "    pred = app.predict(y, image_mpp=1)\n",
    "    prd = app._untile_output(pred, tile_info)\n",
    "    #io.imshow(prd[0,:,:,0])\n",
    "    plt.show()\n",
    "    \n",
    "    APs[CURR_IM_NAME] = syotil.csi(mask_true, prd[0,:,:,0])# masks may lose one pixel if dimension is odd pixels\n",
    "\n",
    "APs[\"mAP\"]=np.mean(list(APs.values()))\n",
    "print(APs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc06419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([FILENAMES+[\"mAP\"], list(APs.values())])\n",
    "print(df.transpose())\n",
    "df.to_csv('images/training/csi_tn_'+label+'.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520fe391",
   "metadata": {},
   "source": [
    "**mAP**<br>\n",
    "image_mpp default: .<br>\n",
    "image_mpp=1: .34<br>\n",
    "image_mpp=2: .<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
