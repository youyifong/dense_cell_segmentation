{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62ebcf14",
   "metadata": {},
   "source": [
    "**Investigate Performance Difference Between CellSeg and Matterport**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "844453f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "mrcnn_path='../Mask_RCNN-TF2'\n",
    "assert os.path.exists(mrcnn_path), 'mrcnn_path does not exist: '+mrcnn_path\n",
    "sys.path.insert(0, \"../CellSeg/src\") # to import CellSeg\n",
    "sys.path.insert(0, mrcnn_path) \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys, datetime, glob,pdb\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "from imgaug import augmenters as iaa\n",
    "from skimage import img_as_ubyte, img_as_uint\n",
    "\n",
    "import syotil\n",
    "\n",
    "# Mask_RCNN-TF2\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "from mrcnn import model as modellib\n",
    "from mrcnn import visualize\n",
    "\n",
    "# CellSeg\n",
    "from cvmodelconfig import CVSegmentationConfig\n",
    "\n",
    "# dense_cell_seg\n",
    "from tfmrcnn_CellsegDataset import *\n",
    "from tfmrcnn_StringerConfig import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb13f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = './' \n",
    "MODELS_DIR = os.path.join(basedir, \"models\")\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "    os.makedirs(MODELS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fe8940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_3dto2d(mask, scores):\n",
    "    \"transform a mask array that is [H, W, count] to [H, W]\"\n",
    "    assert mask.ndim == 3, \"Mask must be [H, W, count]\"\n",
    "    # If mask is empty, return line with image ID only\n",
    "    if mask.shape[-1] == 0:\n",
    "        return np.zeros(mask.shape[:3])\n",
    "    # Remove mask overlaps\n",
    "    # Multiply each instance mask by its score order\n",
    "    # then take the maximum across the last dimension\n",
    "    order = np.argsort(scores)[::-1] + 1  # 1-based descending\n",
    "    mask = np.max(mask * np.reshape(order, [1, 1, -1]), -1)\n",
    "    return mask\n",
    "\n",
    "def remove_overlaps(masks, cellpix, medians):\n",
    "    \"\"\" replace overlapping mask pixels with mask id of closest mask\n",
    "        masks = Nmasks x Ly x Lx\n",
    "    \"\"\"\n",
    "    overlaps = np.array(np.nonzero(cellpix>1.5)).T\n",
    "    dists = ((overlaps[:,:,np.newaxis] - medians.T)**2).sum(axis=1)\n",
    "    tocell = np.argmin(dists, axis=1)\n",
    "    masks[:, overlaps[:,0], overlaps[:,1]] = 0\n",
    "    masks[tocell, overlaps[:,0], overlaps[:,1]] = 1\n",
    "\n",
    "    # labels should be 1 to mask.shape[0]\n",
    "    masks = masks.astype(int) * np.arange(1,masks.shape[0]+1,1,int)[:,np.newaxis,np.newaxis]\n",
    "    masks = masks.sum(axis=0)\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb594839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: images/test_images\n",
      "Results saved to: images/test_tfmrcnn_cellseg2\n",
      "Weights: ../CellSeg/src/modelFiles/final_weights.h5\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Mask R-CNN for cell counting and segmentation')\n",
    "args = argparse.Namespace(gpu_id=1, results_dir=\"images/test_tfmrcnn_cellseg2\", dataset=\"images/test_images\", batch_size=2, weights_path=\"../CellSeg/src/modelFiles/final_weights.h5\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(args.gpu_id)\n",
    "\n",
    "print(\"Running on: {}\".format(args.dataset))\n",
    "    \n",
    "if args.results_dir: \n",
    "    results_dir=args.results_dir\n",
    "else:\n",
    "    results_dir = \"testmasks_{:%Y%m%dT%H%M%S}\".format(datetime.datetime.now())\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "print(\"Results saved to: {}\".format(args.results_dir))\n",
    "\n",
    "print(\"Weights: {}\".format(args.weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0e6cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "dataset = CellsegDataset()\n",
    "dataset.load_data(args.dataset, '')\n",
    "dataset.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ce1b3",
   "metadata": {},
   "source": [
    "**There is a huge difference in performance using different configs as the next two cells show.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcb42f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3166023166023166, 0.25667351129363447, 0.2857142857142857, 0.3076923076923077, 0.29672447013487474, 0.24761904761904763, 0.2558139534883721]\n",
      "mAP: 0.2809771275064056\n"
     ]
    }
   ],
   "source": [
    "config = StringerEvalConfig(); config.NAME = \"cellpose\"\n",
    "model = modellib.MaskRCNN(mode=\"inference\", config=config, model_dir=MODELS_DIR)    \n",
    "model.load_weights(args.weights_path, by_name=True)\n",
    "\n",
    "remove_overlap=True# masks with overlap removed work better\n",
    "AP_arr=[]\n",
    "for image_id in dataset.image_ids:    \n",
    "    image = dataset.load_image(image_id)\n",
    "    r = model.detect([image], verbose=0)[0]\n",
    "    mask = r[\"masks\"]\n",
    "    if remove_overlap:\n",
    "        medians = []\n",
    "        for m in range(mask.shape[-1]):\n",
    "            ypix, xpix = np.nonzero(mask[:,:,m])\n",
    "            medians.append(np.array([ypix.mean(), xpix.mean()]))\n",
    "        mask = np.int32(remove_overlaps(np.transpose(mask, (2,0,1)), mask.sum(axis=-1), np.array(medians)))             \n",
    "    else:\n",
    "        mask = mask_3dto2d(mask, r[\"scores\"])\n",
    "#     skimage.io.imsave(\"{}/{}.png\".format(results_dir, dataset.image_info[image_id][\"id\"].replace(\"_img\",\"_masks\")), \n",
    "#                       img_as_uint(mask), check_contrast=False)\n",
    "\n",
    "    truth=skimage.io.imread(\"images/test_gtmasks/\"+dataset.image_info[image_id][\"id\"].replace(\"img\",\"masks\")+\".png\")\n",
    "    AP_arr.append(syotil.csi(mask, truth))# masks may lost one pixel\n",
    "    \n",
    "print(AP_arr)\n",
    "print(\"mAP: \"+str(np.mean(AP_arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7e20961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/software/fhPython/3.7.12-GCCcore-11.2.0/lib/python3.7/site-packages/skimage/util/dtype.py:454: UserWarning: Downcasting int32 to uint16 without scaling because max value 317 fits in uint16\n",
      "  return convert(image, np.uint16, force_copy)\n",
      "/app/software/fhPython/3.7.12-GCCcore-11.2.0/lib/python3.7/site-packages/skimage/util/dtype.py:454: UserWarning: Downcasting int32 to uint16 without scaling because max value 268 fits in uint16\n",
      "  return convert(image, np.uint16, force_copy)\n",
      "/app/software/fhPython/3.7.12-GCCcore-11.2.0/lib/python3.7/site-packages/skimage/util/dtype.py:454: UserWarning: Downcasting int32 to uint16 without scaling because max value 152 fits in uint16\n",
      "  return convert(image, np.uint16, force_copy)\n",
      "/app/software/fhPython/3.7.12-GCCcore-11.2.0/lib/python3.7/site-packages/skimage/util/dtype.py:454: UserWarning: Downcasting int32 to uint16 without scaling because max value 207 fits in uint16\n",
      "  return convert(image, np.uint16, force_copy)\n",
      "/app/software/fhPython/3.7.12-GCCcore-11.2.0/lib/python3.7/site-packages/skimage/util/dtype.py:454: UserWarning: Downcasting int32 to uint16 without scaling because max value 342 fits in uint16\n",
      "  return convert(image, np.uint16, force_copy)\n",
      "/app/software/fhPython/3.7.12-GCCcore-11.2.0/lib/python3.7/site-packages/skimage/util/dtype.py:454: UserWarning: Downcasting int32 to uint16 without scaling because max value 297 fits in uint16\n",
      "  return convert(image, np.uint16, force_copy)\n",
      "/app/software/fhPython/3.7.12-GCCcore-11.2.0/lib/python3.7/site-packages/skimage/util/dtype.py:454: UserWarning: Downcasting int32 to uint16 without scaling because max value 275 fits in uint16\n",
      "  return convert(image, np.uint16, force_copy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3860294117647059, 0.3617886178861789, 0.4020100502512563, 0.3314285714285714, 0.42401500938086306, 0.4156479217603912, 0.4040632054176072]\n",
      "0.38928325541279624\n"
     ]
    }
   ],
   "source": [
    "config = CVSegmentationConfig(smallest_side=256); config.NAME = \"CellSeg\"; config.PRE_NMS_LIMIT = 6000\n",
    "model = modellib.MaskRCNN(mode=\"inference\", config=config, model_dir=MODELS_DIR)    \n",
    "model.load_weights(args.weights_path, by_name=True)\n",
    "\n",
    "remove_overlap=True # masks with overlap removed work better\n",
    "AP_arr=[]\n",
    "for image_id in dataset.image_ids:    \n",
    "    image = dataset.load_image(image_id)\n",
    "    r = model.detect([image], verbose=0)[0]\n",
    "    mask = r[\"masks\"]\n",
    "    if remove_overlap:\n",
    "        medians = []\n",
    "        for m in range(mask.shape[-1]):\n",
    "            ypix, xpix = np.nonzero(mask[:,:,m])\n",
    "            medians.append(np.array([ypix.mean(), xpix.mean()]))\n",
    "        mask = np.int32(remove_overlaps(np.transpose(mask, (2,0,1)), mask.sum(axis=-1), np.array(medians)))             \n",
    "    else:\n",
    "        mask = mask_3dto2d(mask, r[\"scores\"])\n",
    "#     skimage.io.imsave(\"{}/{}.png\".format(results_dir, dataset.image_info[image_id][\"id\"].replace(\"_img\",\"_masks\")), \n",
    "#                       img_as_uint(mask), check_contrast=False)\n",
    "    truth=skimage.io.imread(\"images/test_gtmasks/\"+dataset.image_info[image_id][\"id\"].replace(\"img\",\"masks\")+\".png\")\n",
    "    AP_arr.append(syotil.csi(mask, truth))# masks may lost one pixel\n",
    "    \n",
    "print(AP_arr)\n",
    "print(\"mAP: \"+str(np.mean(AP_arr)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
