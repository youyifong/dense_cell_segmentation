{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62ebcf14",
   "metadata": {},
   "source": [
    "**Investigate Performance Difference Between CellSeg and Matterport**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "844453f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "mrcnn_path='../Mask_RCNN-TF2'\n",
    "assert os.path.exists(mrcnn_path), 'mrcnn_path does not exist: '+mrcnn_path\n",
    "sys.path.insert(0, \"../CellSeg/src\") # to import CellSeg\n",
    "sys.path.insert(0, mrcnn_path) \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys, datetime, glob,pdb\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "from imgaug import augmenters as iaa\n",
    "from skimage import img_as_ubyte, img_as_uint\n",
    "\n",
    "import syotil\n",
    "\n",
    "# Mask_RCNN-TF2 matterport alsombra\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "from mrcnn import model as modellib\n",
    "from mrcnn import visualize\n",
    "\n",
    "# CellSeg\n",
    "from cvmodelconfig import CVSegmentationConfig\n",
    "\n",
    "# dense_cell_seg\n",
    "from tfmrcnn_CellsegDataset import *\n",
    "from tfmrcnn_StringerConfig import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb13f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = './' \n",
    "MODELS_DIR = os.path.join(basedir, \"models\")\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "    os.makedirs(MODELS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fe8940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_3dto2d(mask, scores):\n",
    "    \"transform a mask array that is [H, W, count] to [H, W]\"\n",
    "    assert mask.ndim == 3, \"Mask must be [H, W, count]\"\n",
    "    # If mask is empty, return line with image ID only\n",
    "    if mask.shape[-1] == 0:\n",
    "        return np.zeros(mask.shape[:3])\n",
    "    # Remove mask overlaps\n",
    "    # Multiply each instance mask by its score order\n",
    "    # then take the maximum across the last dimension\n",
    "    order = np.argsort(scores)[::-1] + 1  # 1-based descending\n",
    "    mask = np.max(mask * np.reshape(order, [1, 1, -1]), -1)\n",
    "    return mask\n",
    "\n",
    "def remove_overlaps(masks, cellpix, medians):\n",
    "    \"\"\" replace overlapping mask pixels with mask id of closest mask\n",
    "        masks = Nmasks x Ly x Lx\n",
    "    \"\"\"\n",
    "    overlaps = np.array(np.nonzero(cellpix>1.5)).T\n",
    "    dists = ((overlaps[:,:,np.newaxis] - medians.T)**2).sum(axis=1)\n",
    "    tocell = np.argmin(dists, axis=1)\n",
    "    masks[:, overlaps[:,0], overlaps[:,1]] = 0\n",
    "    masks[tocell, overlaps[:,0], overlaps[:,1]] = 1\n",
    "\n",
    "    # labels should be 1 to mask.shape[0]\n",
    "    masks = masks.astype(int) * np.arange(1,masks.shape[0]+1,1,int)[:,np.newaxis,np.newaxis]\n",
    "    masks = masks.sum(axis=0)\n",
    "    return masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb594839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: images/test_images\n",
      "Results saved to: images/test_tfmrcnn_cellseg2\n",
      "Weights: ../CellSeg/src/modelFiles/final_weights.h5\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Mask R-CNN for cell counting and segmentation')\n",
    "args = argparse.Namespace(gpu_id=1, results_dir=\"images/test_tfmrcnn_cellseg2\", dataset=\"images/test_images\", batch_size=2, weights_path=\"../CellSeg/src/modelFiles/final_weights.h5\")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(args.gpu_id)\n",
    "\n",
    "print(\"Running on: {}\".format(args.dataset))\n",
    "    \n",
    "if args.results_dir: \n",
    "    results_dir=args.results_dir\n",
    "else:\n",
    "    results_dir = \"testmasks_{:%Y%m%dT%H%M%S}\".format(datetime.datetime.now())\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "print(\"Results saved to: {}\".format(args.results_dir))\n",
    "\n",
    "print(\"Weights: {}\".format(args.weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e6cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "dataset = CellsegDataset()\n",
    "dataset.load_data(args.dataset, '')\n",
    "dataset.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69654b15",
   "metadata": {},
   "source": [
    "**There is a huge difference in performance using different configs as the next two cells show.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c3a7771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /app/software/fhPython/3.7.12-GCCcore-11.2.0/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/software/fhPython/3.7.12-GCCcore-11.2.0/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3166023166023166, 0.25667351129363447, 0.2857142857142857, 0.3076923076923077, 0.29672447013487474, 0.24761904761904763, 0.2558139534883721]\n",
      "mAP: 0.2809771275064056\n"
     ]
    }
   ],
   "source": [
    "config1 = StringerEvalConfig()\n",
    "config1.NAME = \"cellpose\"\n",
    "\n",
    "model = modellib.MaskRCNN(mode=\"inference\", config=config1, model_dir=MODELS_DIR)    \n",
    "model.load_weights(args.weights_path, by_name=True)\n",
    "\n",
    "remove_overlap=True# masks with overlap removed work better\n",
    "AP_arr=[]\n",
    "for image_id in dataset.image_ids:    \n",
    "    image = dataset.load_image(image_id)\n",
    "    r = model.detect([image], verbose=0)[0]\n",
    "    mask = r[\"masks\"]\n",
    "    if remove_overlap:\n",
    "        medians = []\n",
    "        for m in range(mask.shape[-1]):\n",
    "            ypix, xpix = np.nonzero(mask[:,:,m])\n",
    "            medians.append(np.array([ypix.mean(), xpix.mean()]))\n",
    "        mask = np.int32(remove_overlaps(np.transpose(mask, (2,0,1)), mask.sum(axis=-1), np.array(medians)))             \n",
    "    else:\n",
    "        mask = mask_3dto2d(mask, r[\"scores\"])\n",
    "#     skimage.io.imsave(\"{}/{}.png\".format(results_dir, dataset.image_info[image_id][\"id\"].replace(\"_img\",\"_masks\")), \n",
    "#                       img_as_uint(mask), check_contrast=False)\n",
    "\n",
    "    truth=skimage.io.imread(\"images/test_gtmasks/\"+dataset.image_info[image_id][\"id\"].replace(\"img\",\"masks\")+\".png\")\n",
    "    AP_arr.append(syotil.csi(mask, truth))# masks may lost one pixel\n",
    "    \n",
    "print(AP_arr)\n",
    "print(\"mAP: \"+str(np.mean(AP_arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7e20961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3860294117647059, 0.3617886178861789, 0.4020100502512563, 0.3314285714285714, 0.42401500938086306, 0.4156479217603912, 0.4040632054176072]\n",
      "mAP: 0.38928325541279624\n"
     ]
    }
   ],
   "source": [
    "# CellSeg configuration\n",
    "config2 = CVSegmentationConfig(smallest_side=256)\n",
    "config2.NAME = \"CellSeg\"\n",
    "config2.PRE_NMS_LIMIT = 6000\n",
    "\n",
    "model = modellib.MaskRCNN(mode=\"inference\", config=config2, model_dir=MODELS_DIR)    \n",
    "model.load_weights(args.weights_path, by_name=True)\n",
    "\n",
    "remove_overlap=True # masks with overlap removed work better\n",
    "AP_arr=[]\n",
    "for image_id in dataset.image_ids:    \n",
    "    image = dataset.load_image(image_id)\n",
    "    r = model.detect([image], verbose=0)[0]\n",
    "    mask = r[\"masks\"]\n",
    "    if remove_overlap:\n",
    "        medians = []\n",
    "        for m in range(mask.shape[-1]):\n",
    "            ypix, xpix = np.nonzero(mask[:,:,m])\n",
    "            medians.append(np.array([ypix.mean(), xpix.mean()]))\n",
    "        mask = np.int32(remove_overlaps(np.transpose(mask, (2,0,1)), mask.sum(axis=-1), np.array(medians)))             \n",
    "    else:\n",
    "        mask = mask_3dto2d(mask, r[\"scores\"])\n",
    "#     skimage.io.imsave(\"{}/{}.png\".format(results_dir, dataset.image_info[image_id][\"id\"].replace(\"_img\",\"_masks\")), \n",
    "#                       img_as_uint(mask), check_contrast=False)\n",
    "    truth=skimage.io.imread(\"images/test_gtmasks/\"+dataset.image_info[image_id][\"id\"].replace(\"img\",\"masks\")+\".png\")\n",
    "    AP_arr.append(syotil.csi(mask, truth))# masks may lost one pixel\n",
    "    \n",
    "print(AP_arr)\n",
    "print(\"mAP: \"+str(np.mean(AP_arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec3a4d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "CPU_COUNT                      0\n",
      "DETECTION_MAX_INSTANCES        400\n",
      "DETECTION_MIN_CONFIDENCE       0\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1024\n",
      "IMAGE_META_SIZE                14\n",
      "IMAGE_MIN_DIM                  256\n",
      "IMAGE_MIN_SCALE                2.0\n",
      "IMAGE_RESIZE_MODE              pad64\n",
      "IMAGE_SHAPE                    [1024 1024    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               200\n",
      "MEAN_PIXEL                     [123.7, 116.8, 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           cellpose\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        2000\n",
      "POST_NMS_ROIS_TRAINING         1000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    1500\n",
      "STEPS_PER_EPOCH                1000\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           300\n",
      "USE_MINI_MASK                  False\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config3.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c18abd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3786008230452675, 0.32, 0.4489795918367347, 0.3409090909090909, 0.39787234042553193, 0.36049382716049383, 0.34438775510204084]\n",
      "mAP: 0.3701776326398799\n"
     ]
    }
   ],
   "source": [
    "config3 = StringerEvalConfig()\n",
    "config3.NAME = \"cellpose\"\n",
    "# 0.2809771275064056\n",
    "\n",
    "# following are the modifications based on config2 (CellSeg)\n",
    "\n",
    "config3.BACKBONE                       = \"resnet101\" # changed from resnet50\n",
    "# 0.3445496033869464 \n",
    "\n",
    "config3.MEAN_PIXEL                     = [123.7, 116.8, 103.9] # changed from [43.53 39.56 48.22]\n",
    "# 0.38641245939340807 \n",
    "\n",
    "config3.DETECTION_MIN_CONFIDENCE       = 0.7 # changed from 0\n",
    "# 0.4101849700393826 \n",
    "\n",
    "# these do not make material differences\n",
    "config3.DETECTION_MAX_INSTANCES        = 2000\n",
    "config3.POST_NMS_ROIS_INFERENCE        = 6000\n",
    "config3.POST_NMS_ROIS_TRAINING         = 2000\n",
    "config3.RPN_NMS_THRESHOLD              = 0.8\n",
    "config3.RPN_TRAIN_ANCHORS_PER_IMAGE    = 256\n",
    "config3.TRAIN_ROIS_PER_IMAGE           = 200\n",
    "config3.ASPECT_RATIO                   = 1\n",
    "config3.MIN_ENLARGE                    = 1\n",
    "config3.ZOOM                           = False\n",
    "config3.IMAGE_SHAPE                    = [0, 0, 3]\n",
    "# 0.41208502417927967\n",
    "\n",
    "# config3.IMAGE_MIN_SCALE                = False\n",
    "# # 0.29976782879444375\n",
    "# # It is 2 by default. CellSeg probably have it as False in the config, but change it in the code\n",
    "\n",
    "\n",
    "model = modellib.MaskRCNN(mode=\"inference\", config=config3, model_dir=MODELS_DIR)    \n",
    "model.load_weights(args.weights_path, by_name=True)\n",
    "\n",
    "remove_overlap=True# masks with overlap removed work better\n",
    "AP_arr=[]\n",
    "for image_id in dataset.image_ids:    \n",
    "    image = dataset.load_image(image_id)\n",
    "    r = model.detect([image], verbose=0)[0]\n",
    "    mask = r[\"masks\"]\n",
    "    if remove_overlap:\n",
    "        medians = []\n",
    "        for m in range(mask.shape[-1]):\n",
    "            ypix, xpix = np.nonzero(mask[:,:,m])\n",
    "            medians.append(np.array([ypix.mean(), xpix.mean()]))\n",
    "        mask = np.int32(remove_overlaps(np.transpose(mask, (2,0,1)), mask.sum(axis=-1), np.array(medians)))             \n",
    "    else:\n",
    "        mask = mask_3dto2d(mask, r[\"scores\"])\n",
    "        \n",
    "#     skimage.io.imsave(\"{}/{}.png\".format(results_dir, dataset.image_info[image_id][\"id\"].replace(\"_img\",\"_masks\")), \n",
    "#                       img_as_uint(mask), check_contrast=False)\n",
    "\n",
    "    truth=skimage.io.imread(\"images/test_gtmasks/\"+dataset.image_info[image_id][\"id\"].replace(\"img\",\"masks\")+\".png\")\n",
    "    AP_arr.append(syotil.csi(mask, truth))# masks may lost one pixel\n",
    "    \n",
    "print(AP_arr)\n",
    "print(\"mAP: \"+str(np.mean(AP_arr)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
